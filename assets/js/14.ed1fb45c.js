(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{715:function(t,s,a){"use strict";a.r(s);var n=a(7),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[a("a",{attrs:{href:"https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching",target:"_blank",rel:"noopener noreferrer"}},[t._v("PatentPhrase Competition"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"数据集的特点"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#数据集的特点"}},[t._v("#")]),t._v(" 数据集的特点")]),t._v(" "),a("p",[t._v("观察数据析发现，人为标注的分数其实是离散的，因此可以用一个"),a("strong",[t._v("分类模型")]),t._v("来预测分数，目前想的是5分类或者6分类，分类数目如果太多，一旦错误可能会导致误差很大。")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kuroweb.tk/picture/16542561862046630.jpg",alt:""}})]),t._v(" "),a("p",[t._v("同时可以观察到，训练集的socore分布"),a("strong",[t._v("偏差比较大")]),t._v("，只有很少的一部分靠于1，由于不知道最终数据，这里也许可以尝试一下")]),t._v(" "),a("p",[t._v("对于 context 字段，根据别人的baseline，每一个编号有对应语言的含义，直接将他们预先保存在字典中，然后展开成为句子，输入到语言模型中，训练他们的句子向量。")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("{'A01': 'HUMAN NECESSITIES. GRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTING; TRAPPING; FISHING',\n 'A21': 'HUMAN NECESSITIES. BAKING; EDIBLE DOUGHS',\n 'A22': 'HUMAN NECESSITIES. BUTCHERING; MEAT TREATMENT; PROCESSING POULTRY OR FISH',\n 'A23': 'HUMAN NECESSITIES. FOODS OR FOODSTUFFS; TREATMENT THEREOF, NOT COVERED BY OTHER CLASSES',\n 'A24': \"HUMAN NECESSITIES. TOBACCO; CIGARS; CIGARETTES; SIMULATED SMOKING DEVICES; SMOKERS' REQUISITES\",\n 'A41': 'HUMAN NECESSITIES. WEARING APPAREL',\n 'A42': 'HUMAN NECESSITIES. HEADWEAR',\n 'A43': 'HUMAN NECESSITIES. FOOTWEAR',\n 'A44': 'HUMAN NECESSITIES. HABERDASHERY; JEWELLERY',\n 'A45': 'HUMAN NECESSITIES. HAND OR TRAVELLING ARTICLES',\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])]),a("p",[t._v("对于这类的词语，也许还可以精心设计——就像代码编号转字符串一样，"),a("strong",[t._v("继续把词语展开")]),t._v("，找到更多分类代码之间"),a("strong",[t._v("词语级别上更直接的关系")]),t._v("，"),a("strong",[t._v("或者手动处理")]),t._v("。")]),t._v(" "),a("p",[t._v("不过这个可能没有什么用？因为用于训练模型的话，也相当于是把外部词语拿过来用了。但是从A01->人类必需品的转换，是必须的。即使如此，上述的方法也值得尝试。")]),t._v(" "),a("p",[t._v("A01,G02这种分类应该可以用机器学习在原始的代码分类上操作。")]),t._v(" "),a("h2",{attrs:{id:"baseline的做法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#baseline的做法"}},[t._v("#")]),t._v(" baseline的做法")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("将上述相关的句子全部直接作为输入，所有需要的信息都在这里的上下文中，然后训练他们的输出分数。完全端到端的处理。目前上面这部分代码，堆积了三个模型，一共花费大约两个小时，平均下来每隔"),a("strong",[t._v("大概半个多小时")]),t._v("。这里的大部分时间依然在模型推理上。")]),t._v(" "),a("h2",{attrs:{id:"ml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ml"}},[t._v("#")]),t._v(" ML")]),t._v(" "),a("p",[t._v("此后接上ml的方法，应该不需要重新训练，因为已经在本数据集上训练过。")]),t._v("\nml花费的时间 = pretrained预处理时间 + ml分类时间 \n\n"),a("p",[t._v("预训练大模型处理的时间比较长，所以大概会和之前的模型持平，也在半个小时左右。")]),t._v(" "),a("h2",{attrs:{id:"model-building"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#model-building"}},[t._v("#")]),t._v(" Model building")]),t._v(" "),a("p",[t._v("使用多种不同的预训练模型向量化文本，然后将其输入到不同的机器学习模型中。")]),t._v(" "),a("p",[t._v("代码编写的时候要注意可以容易替换，可以省下不少工作。")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kuroweb.tk/picture/16542568140600260.jpg",alt:""}})]),t._v(" "),a("p",[t._v("在剩下的一些操作，比如第一列用vectorizor1，第2列用vectorizor2，然后做分类")]),t._v(" "),a("p",[t._v("ml model的组件:")]),t._v(" "),a("p",[t._v("xgboost , lightgbm , svr , mlp , cnn")]),t._v(" "),a("h2",{attrs:{id:"还是关于专利分类编号处理的问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#还是关于专利分类编号处理的问题"}},[t._v("#")]),t._v(" 还是关于专利分类编号处理的问题")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("prepare_input")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cfg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    adb"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    inputs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cfg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tokenizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           max_length"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cfg"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_len"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           padding"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                           truncation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" v "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("items"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("k"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("v"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" inputs\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("p",[t._v("在预训练向量化的过程中，他使用了截断处理，并且设置max_len = 130，然而，有相当一部分数值超过了130。针对truncate参数的"),a("a",{attrs:{href:"https://discuss.huggingface.co/t/purpose-of-padding-and-truncating/412/5",target:"_blank",rel:"noopener noreferrer"}},[t._v("讲解"),a("OutboundLink")],1),t._v("，可见超过的部分根本没有利用上这部分句子的信息，而是应该由用户来把握句子的长度。因此有两种办法，一种是把130提高，另一种是手动把句子才减到130以内。作者设置成130，可能是因为时间的考量，"),a("strong",[t._v("因此手动截取句子可能比较好")]),t._v("，去掉那些不太重要的含义，而句子长度不够的，也可以补充一些词汇。")]),t._v(" "),a("p",[t._v("即使如此，句子的长度依然是不定长的，因此"),a("strong",[t._v("把不重要的关键词放在后面")]),t._v("，即使剔除了也无所谓的那种。同时"),a("strong",[t._v("把说明文字少的分类补充补充，避免token的浪费")]),t._v("。")]),t._v(" "),a("h2",{attrs:{id:"一种提速方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#一种提速方法"}},[t._v("#")]),t._v(" 一种提速方法")]),t._v(" "),a("p",[t._v("由于目前的baseline用的是这种方法")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("test['anchor'] + '[SEP]' + test['target'] + '[SEP]'  + test['context_text']\n\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("p",[t._v("对于每个句子的上下文信息都要重复推断一次，造成了时间上的浪费。应该可以拆解成如下的形式")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("1.test['anchor'] + '[SEP]' + test['target']   \n\n2.test['context_text']\n\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br")])]),a("p",[t._v("然后将 1.2  融合 ， 2 的部分直接"),a("strong",[t._v("提前存储成map形式")]),t._v("。1的部分允许deberta减小tokenizer max_len，可以提高速度。"),a("strong",[t._v("2的部分精心训练")])]),t._v(" "),a("h2",{attrs:{id:"cos-sim"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cos-sim"}},[t._v("#")]),t._v(" cos sim")]),t._v(" "),a("p",[t._v("目前用的是端到端的处理，最后的分数计算用的是mlp，可以改成余弦相似度")]),t._v(" "),a("p",[a("img",{attrs:{src:"http://kuroweb.tk/picture/16542706444440894.jpg",alt:""}})]),t._v(" "),a("h2",{attrs:{id:"fake-data-cleaning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fake-data-cleaning"}},[t._v("#")]),t._v(" fake data cleaning")]),t._v(" "),a("p",[t._v("人为清除掉训练集中错误的数据")]),t._v(" "),a("h2",{attrs:{id:"vectorizer选择"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vectorizer选择"}},[t._v("#")]),t._v(" vectorizer选择")]),t._v(" "),a("p",[t._v("https://www.kaggle.com/general/201825")]),t._v(" "),a("h2",{attrs:{id:"如何得到一个好的向量表示"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#如何得到一个好的向量表示"}},[t._v("#")]),t._v(" 如何得到一个好的向量表示?")]),t._v(" "),a("p",[t._v("必然要finetune，但是根据什么?")]),t._v(" "),a("h2",{attrs:{id:"更多的信息"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#更多的信息"}},[t._v("#")]),t._v(" 更多的信息?")]),t._v(" "),a("p",[t._v("如何在"),a("a",{attrs:{href:"https://www.kaggle.com/datasets/xhlulu/cpc-codes",target:"_blank",rel:"noopener noreferrer"}},[t._v("此表格"),a("OutboundLink")],1),t._v("的基础上挖掘更多的信息，比如给"),a("strong",[t._v("类做pca")]),t._v("。加一个聚类表示")]),t._v(" "),a("h2",{attrs:{id:"模型挑选"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#模型挑选"}},[t._v("#")]),t._v(" 模型挑选")]),t._v(" "),a("p",[t._v("https://www.kaggle.com/code/tanlikesmath/pretrained-sentence-transformer-model-baseline")]),t._v(" "),a("h2",{attrs:{id:"参考文章"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考文章"}},[t._v("#")]),t._v(" 参考文章")]),t._v(" "),a("p",[t._v("https://www.kaggle.com/competitions/petfinder-pawpularity-score/discussion/288896")])])}),[],!1,null,null,null);s.default=e.exports}}]);